import { useState, useRef, useEffect } from "react";
import { voiceApi } from "../api/voice";
import { connectRealtimeVoice, type VoiceConnection } from "../lib/realtime";
import type { CHARACTER_LIST } from "../../character/characters";

export interface UseVoiceConnectionOptions {
  speechLang: "ko" | "en";
  echoCancellation: boolean;
  noiseSuppression: boolean;
  autoGainControl: boolean;
  selectedVoice: string;
  personaCharacter: (typeof CHARACTER_LIST)[number];
  personaGender: "male" | "female";
  onUserMessage?: (text: string) => void;
  onAssistantMessage?: (text: string) => void;
  onUserSpeechStart?: () => void;
  onUserTranscriptUpdate?: (text: string, isFinal: boolean) => void;
}

export interface UseVoiceConnectionReturn {
  // ìƒíƒœë“¤
  voiceEnabled: boolean;
  isRecording: boolean;
  isListening: boolean;
  isResponding: boolean;

  voiceConn: VoiceConnection | null;
  audioRef: React.RefObject<HTMLAudioElement | null>;

  // ì•¡ì…˜ë“¤
  startVoice: () => Promise<void>;
  stopVoice: () => void;
  updatePersona: () => void; // ì‹¤ì‹œê°„ í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸ ì¶”ê°€

  setVoiceEnabled: (enabled: boolean) => void;
  sendVoiceMessage: (message: string) => void;
}

/**
 * OpenAI Realtime API ê¸°ë°˜ ì‹¤ì‹œê°„ AI ìŒì„± ëŒ€í™” í›…
 *
 * ì‚¬ìš©ì ìŒì„± â†’ ìŒì„±ì¸ì‹ â†’ AI ì‘ë‹µ ìƒì„± â†’ ìŒì„±í•©ì„± â†’ ì¬ìƒ
 * ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í›…
 */
export const useVoiceConnection = (
  options: UseVoiceConnectionOptions,
): UseVoiceConnectionReturn => {
  const {
    speechLang,
    echoCancellation,
    noiseSuppression,
    autoGainControl,
    selectedVoice,
    personaCharacter,
    personaGender,
    onUserMessage,
    onAssistantMessage,
    onUserSpeechStart,
    onUserTranscriptUpdate,
  } = options;

  // ìŒì„± ì—°ê²° ìƒíƒœë“¤
  const [voiceEnabled, setVoiceEnabled] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [voiceConn, setVoiceConn] = useState<VoiceConnection | null>(null);
  const [isListening, setIsListening] = useState(false);
  const [isResponding, setIsResponding] = useState(false);

  // Refs
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const isRespondingRef = useRef(false);
  const assistantPartialRef = useRef<string>("");
  const lastUserFinalRef = useRef<string>("");
  const lastAssistantFinalRef = useRef<string>("");

  // í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜
  const normalizeText = (s: string) => {
    try {
      if (!s || typeof s !== "string") return "";
      let normalized = s.normalize("NFC");
      normalized = normalized.replace(/[\uFFFD\u0000-\u001F]/g, "");
      normalized = normalized.replace(/\s+/g, " ").trim();
      return normalized;
    } catch (error) {
      console.warn("í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤íŒ¨:", error);
      return (s || "").trim();
    }
  };

  // AI í˜ë¥´ì†Œë‚˜ ì§€ì‹œì‚¬í•­ ìƒì„±
  const buildPersonaInstructions = () => {
    console.log("ğŸ­ [buildPersonaInstructions] í˜„ì¬ ìºë¦­í„° ìƒíƒœ:");
    console.log("  - personaCharacter:", personaCharacter);
    console.log("  - personaGender:", personaGender);
    console.log("  - selectedVoice:", selectedVoice);
    console.log(
      "  - personaCharacter.personality:",
      personaCharacter?.personality,
    );
    console.log(
      "  - personaCharacter.background:",
      personaCharacter?.background,
    );
    console.log(
      "  - personaCharacter.firstMessage:",
      personaCharacter?.firstMessage,
    );

    const genderNote =
      personaGender === "male"
        ? "ë‚¨ì„±ì ì¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
        : "ì—¬ì„±ì ì¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ";
    const voiceNote = selectedVoice ? `ìŒì„±: ${selectedVoice}. ` : "";
    const personaSummary = personaCharacter
      ? `${personaCharacter.personality}\n${personaCharacter.background}`
      : "";

    // ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ ì—­í• ê·¹ ëª¨ë“œ
    const languageNote =
      "ìºë¦­í„° ì—­í• ì„ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì˜ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë§í•˜ë©´ ê·¸ ì˜ë„ë¥¼ ì´í•´í•˜ë˜, ë‹¹ì‹ ì˜ ìºë¦­í„°ê°€ í•˜ë“¯ì´ ì˜ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì§ì ‘ì ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”. ë‹µë³€ì€ ìµœëŒ€ 1-2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”. ";

    const characterEmphasis = personaCharacter
      ? `ë‹¹ì‹ ì€ ${personaCharacter.name}ì…ë‹ˆë‹¤. ${personaSummary} `
      : "";

    const firstMessageNote = personaCharacter.firstMessage
      ? `ì²« ë²ˆì§¸ ë©”ì‹œì§€ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤: "${personaCharacter.firstMessage}". `
      : "";

    // ë³µì¡í•œ ìºë¦­í„°ë³„ ì§€ì¹¨ ì œê±°ë¨

    const finalInstructions =
      `ë‚˜ëŠ” ${personaCharacter.name} (${personaCharacter.emoji})ì…ë‹ˆë‹¤. ${genderNote}${voiceNote}` +
      firstMessageNote +
      characterEmphasis +
      languageNote +
      `ë‚˜ëŠ” í•­ìƒ ìºë¦­í„° ì—­í• ì„ ìœ ì§€í•©ë‹ˆë‹¤`;

    console.log("ğŸ­ [buildPersonaInstructions] ìƒì„±ëœ ì§€ì‹œì‚¬í•­:");
    console.log(finalInstructions);

    return finalInstructions;
  };

  // ìŒì„± ì—°ê²° ì‹œì‘
  const startVoice = async () => {
    console.log("ğŸ­ [startVoice] ìŒì„± ì—°ê²° ì‹œì‘ í˜¸ì¶œë¨");
    if (voiceConn) {
      console.log("ğŸ­ [startVoice] ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆì–´ì„œ ì¢…ë£Œ");
      return;
    }

    try {
      const session = await voiceApi.createSession({
        lang: speechLang,
        voice: selectedVoice,
      });

      let micStream: MediaStream | null = null;

      const conn = await connectRealtimeVoice({
        token: session.token,
        model: session.model,
        audioElement: audioRef.current,
        voice: selectedVoice,
        audioConstraints: {
          echoCancellation,
          noiseSuppression,
          autoGainControl,
          channelCount: 1,
        },
        onEvent: (evt) => {
          const e: any = evt as any;
          const t = e?.type as string | undefined;
          if (t === "input_audio_buffer.speech_started") {
            setIsListening(true);
            onUserSpeechStart?.(); // ìŒì„± ì‹œì‘ ì•Œë¦¼
          }
          if (t === "input_audio_buffer.speech_stopped") setIsListening(false);
          if (t === "output_audio_buffer.started") {
            setIsResponding(true);
            isRespondingRef.current = true;
            try {
              micStream
                ?.getAudioTracks()
                ?.forEach((tr) => (tr.enabled = false));
            } catch {}
          }
          if (t === "response.done" || t === "output_audio_buffer.stopped") {
            setIsResponding(false);
            isRespondingRef.current = false;
            try {
              micStream?.getAudioTracks()?.forEach((tr) => (tr.enabled = true));
            } catch {}
          }
        },
        onUserTranscript: (text, isFinal) => {
          if (isRespondingRef.current) return; // ì–´ì‹œìŠ¤í„´íŠ¸ ë°œí™” ì¤‘ ì „ì‚¬ ë¬´ì‹œ

          // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (finalì´ ì•„ë‹ˆì–´ë„ í˜¸ì¶œ)
          onUserTranscriptUpdate?.(text, isFinal);

          if (isFinal) {
            const finalText = normalizeText(text.trim());
            console.log("[ìŒì„± ë””ë²„ê·¸] ìµœì¢… í…ìŠ¤íŠ¸:", finalText);
            console.log("[ìŒì„± ë””ë²„ê·¸] ì´ì „ í…ìŠ¤íŠ¸:", lastUserFinalRef.current);

            // í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¶”ê°€ (ì¤‘ë³µ ì²´í¬ ì™„í™”)
            if (finalText && finalText.length > 0) {
              console.log("[ìŒì„± ë””ë²„ê·¸] ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ì¤‘:", finalText);
              // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì™¸ë¶€ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
              onUserMessage?.(finalText);
              lastUserFinalRef.current = finalText;

              // create_response: falseì´ë¯€ë¡œ ìˆ˜ë™ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (ì§€ì—° í›„)
              setTimeout(() => {
                try {
                  conn?.dc?.send(
                    JSON.stringify({
                      type: "response.create",
                      response: {
                        modalities: ["text", "audio"],
                        instructions: "Please respond naturally and helpfully.",
                      },
                    }),
                  );
                  console.log("[ìŒì„± ë””ë²„ê·¸] ì‘ë‹µ ìƒì„± ìš”ì²­ ì „ì†¡");
                } catch (e) {
                  console.error("[ìŒì„± ë””ë²„ê·¸] ì‘ë‹µ ìƒì„± ìš”ì²­ ì‹¤íŒ¨:", e);
                }
              }, 1500); // 1.5ì´ˆ ì§€ì—° í›„ ì‘ë‹µ ìƒì„±
            } else {
              console.log("[ìŒì„± ë””ë²„ê·¸] í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ì„œ ìŠ¤í‚µ");
            }
          }
        },
        onAssistantText: (text, isFinal) => {
          if (isFinal) {
            const finalText = normalizeText(
              assistantPartialRef.current || text,
            );
            const normalizedFinal = normalizeText(finalText);
            const normalizedLast = normalizeText(lastAssistantFinalRef.current);
            if (normalizedFinal && normalizedFinal !== normalizedLast) {
              // AI ë©”ì‹œì§€ë¥¼ ì™¸ë¶€ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
              onAssistantMessage?.(finalText);
              lastAssistantFinalRef.current = finalText.trim();
            }
            assistantPartialRef.current = "";
          } else {
            assistantPartialRef.current += text;
          }
        },
      });

      setVoiceConn(conn);
      try {
        micStream = conn.localStream;
      } catch {}
      setIsRecording(true);

      // ì„¸ì…˜ ê¸°ë³¸ í¼ì†Œë‚˜ ì—…ë°ì´íŠ¸
      try {
        const instructions = buildPersonaInstructions();
        const sessionConfig = {
          type: "session.update",
          session: {
            instructions: instructions,
            turn_detection: {
              type: "server_vad",
              threshold: 0.6, // ë” í™•ì‹¤í•œ ìŒì„±ë§Œ ê°ì§€ (ì¡ìŒ ë¬´ì‹œ)
              prefix_padding_ms: 300, // ìŒì„± ì‹œì‘ ì „ ì—¬ìœ ì‹œê°„
              silence_duration_ms: 800, // ì¹¨ë¬µ 800ms í›„ í„´ ì¢…ë£Œ ê°ì§€
              create_response: false, // ìë™ ì‘ë‹µ ë¹„í™œì„±í™” (ìˆ˜ë™ ì œì–´)
            },
          },
        };

        console.log("ğŸ­ [startVoice] OpenAI ì„¸ì…˜ ì„¤ì • ì „ì†¡:");
        console.log(JSON.stringify(sessionConfig, null, 2));

        if (conn.dc && conn.dc.readyState === "open") {
          conn.dc.send(JSON.stringify(sessionConfig));
          console.log("ğŸ­ [startVoice] ì¦‰ì‹œ ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì „ì†¡ ì™„ë£Œ");
        } else {
          console.log(
            "ğŸ­ [startVoice] ë°ì´í„° ì±„ë„ ëŒ€ê¸° ì¤‘, open ì´ë²¤íŠ¸ì—ì„œ ì „ì†¡ ì˜ˆì •",
          );
          conn.dc?.addEventListener("open", () => {
            try {
              console.log(
                "ğŸ­ [startVoice] open ì´ë²¤íŠ¸ ë°œìƒ - ìƒˆë¡œìš´ ì„¸ì…˜ ì„¤ì • ìƒì„±",
              );
              const newInstructions = buildPersonaInstructions();
              const newSessionConfig = {
                type: "session.update",
                session: {
                  instructions: newInstructions,
                  turn_detection: {
                    type: "server_vad",
                    threshold: 0.6,
                    prefix_padding_ms: 300,
                    silence_duration_ms: 800,
                    create_response: false,
                  },
                },
              };

              console.log("ğŸ­ [startVoice] open ì´ë²¤íŠ¸ì—ì„œ ìƒˆë¡œ ìƒì„±í•œ ì„¤ì •:");
              console.log(JSON.stringify(newSessionConfig, null, 2));

              conn.dc?.send(JSON.stringify(newSessionConfig));
              console.log(
                "ğŸ­ [startVoice] open ì´ë²¤íŠ¸ì—ì„œ ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì „ì†¡ ì™„ë£Œ",
              );
            } catch (e) {
              console.error(
                "ğŸ­ [startVoice] open ì´ë²¤íŠ¸ì—ì„œ ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:",
                e,
              );
            }
          });
        }
      } catch (e) {
        console.error("ğŸ­ [startVoice] ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì¤€ë¹„ ì‹¤íŒ¨:", e);
      }
    } catch (e) {
      console.error("ìŒì„± ì—°ê²° ì‹¤íŒ¨:", e);
    }
  };

  // ìŒì„± ì—°ê²° ì¢…ë£Œ
  const stopVoice = () => {
    try {
      voiceConn?.stop();
    } catch {}
    setVoiceConn(null);
    setIsRecording(false);
    setIsListening(false);
    setIsResponding(false);
  };

  // ì‹¤ì‹œê°„ í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸
  const updatePersona = () => {
    try {
      if (voiceConn?.dc && voiceConn.dc.readyState === "open") {
        const newInstructions = buildPersonaInstructions();
        console.log(
          "ğŸ­ [í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸] ìƒˆë¡œìš´ ì§€ì‹œì‚¬í•­ ì „ì†¡:",
          newInstructions,
        );

        voiceConn.dc.send(
          JSON.stringify({
            type: "session.update",
            session: {
              instructions: newInstructions,
              turn_detection: {
                type: "server_vad",
                threshold: 0.6,
                prefix_padding_ms: 300,
                silence_duration_ms: 800,
                create_response: false,
              },
            },
          }),
        );

        console.log("ğŸ­ [í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸] OpenAI ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ");
      } else {
        console.warn("ğŸ­ [í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸] ìŒì„± ì—°ê²°ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ");
      }
    } catch (e) {
      console.error("ğŸ­ [í˜ë¥´ì†Œë‚˜ ì—…ë°ì´íŠ¸] ì‹¤íŒ¨:", e);
    }
  };

  // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ìŒì„± ì—°ê²°ë¡œ ì „ì†¡
  const sendVoiceMessage = (message: string) => {
    try {
      if (voiceConn?.dc && voiceConn.dc.readyState === "open") {
        voiceConn.dc.send(
          JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: "user",
              content: [{ type: "input_text", text: message }],
            },
          }),
        );
        voiceConn.dc.send(
          JSON.stringify({
            type: "response.create",
            response: {
              modalities: ["audio", "text"],
              conversation: "auto",
              voice: selectedVoice,
            },
          }),
        );
      }
    } catch (e) {
      console.error("ìŒì„± ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨:", e);
    }
  };

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopVoice();
    };
  }, []);

  return {
    // ìƒíƒœë“¤
    voiceEnabled,
    isRecording,
    isListening,
    isResponding,
    voiceConn,
    audioRef,

    // ì•¡ì…˜ë“¤
    startVoice,
    stopVoice,
    updatePersona,
    setVoiceEnabled,
    sendVoiceMessage,
  };
};
