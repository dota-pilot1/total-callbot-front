import { useState, useRef, useCallback, useEffect } from "react";
import { voiceApi } from "../../chatbot/voice/api/voice";
import {
  connectRealtimeVoice,
  type VoiceConnection,
} from "../../chatbot/voice/lib/realtime";

export interface UseVoiceToTextOptions {
  onTranscript?: (text: string, isFinal: boolean) => void;
  onError?: (error: string) => void;
}

export interface UseVoiceToTextReturn {
  isRecording: boolean;
  isListening: boolean;
  transcriptText: string;
  startRecording: () => Promise<void>;
  stopRecording: () => void;
  clearTranscript: () => void;
}

/**
 * ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì „ìš© í›… (ëŒ€í™” ì—†ìŒ)
 * OpenAI Realtime API ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ìš°ì €/ëª¨ë°”ì¼ ëª¨ë‘ ì§€ì›
 */
export const useVoiceToText = (
  options: UseVoiceToTextOptions = {},
): UseVoiceToTextReturn => {
  const { onTranscript, onError } = options;

  const [isRecording, setIsRecording] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [transcriptText, setTranscriptText] = useState("");

  const voiceConnRef = useRef<VoiceConnection | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);

  const startRecording = useCallback(async () => {
    if (isRecording) return;

    try {
      console.log("ðŸŽ¤ Starting voice-to-text recording...");

      // Create session for voice recognition only
      const session = await voiceApi.createSession({
        lang: "ko",
        voice: "alloy", // ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
      });

      const conn = await connectRealtimeVoice({
        token: session.token,
        model: session.model,
        audioElement: audioRef.current,
        voice: "alloy",
        audioConstraints: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1,
        },
        // ìŒì„± ìž…ë ¥ ê°ì§€
        onEvent: (evt) => {
          const e: any = evt as any;
          const t = e?.type as string | undefined;
          if (t === "input_audio_buffer.speech_started") {
            console.log("ðŸŽ¤ Speech detected");
            setIsListening(true);
          }
          if (t === "input_audio_buffer.speech_stopped") {
            console.log("ðŸŽ¤ Speech ended");
            setIsListening(false);
          }
        },
        // ì‚¬ìš©ìž ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼
        onUserTranscript: (text, isFinal) => {
          console.log("ðŸ”¤ Transcript:", text, "isFinal:", isFinal);

          if (isFinal) {
            setTranscriptText((prev) => {
              const newText = prev + (prev ? " " : "") + text;
              onTranscript?.(newText, true);
              return newText;
            });
          } else {
            // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            onTranscript?.(text, false);
          }
        },
        // AI ì‘ë‹µì€ ë¬´ì‹œ (í…ìŠ¤íŠ¸ ë³€í™˜ ì „ìš©ì´ë¯€ë¡œ)
        onAssistantText: () => {},
      });

      voiceConnRef.current = conn;
      setIsRecording(true);

      // ìŒì„± ì¸ì‹ ì „ìš©ìœ¼ë¡œ ì„¤ì • (AI ì‘ë‹µ ë¹„í™œì„±í™”)
      try {
        if (conn.dc && conn.dc.readyState === "open") {
          conn.dc.send(
            JSON.stringify({
              type: "session.update",
              session: {
                instructions:
                  "You are a voice transcription service. Do not respond to user input, just transcribe.",
                turn_detection: { type: "server_vad" },
                input_audio_transcription: { model: "whisper-1" },
              },
            }),
          );
        } else {
          conn.dc?.addEventListener("open", () => {
            try {
              conn.dc?.send(
                JSON.stringify({
                  type: "session.update",
                  session: {
                    instructions:
                      "You are a voice transcription service. Do not respond to user input, just transcribe.",
                    turn_detection: { type: "server_vad" },
                    input_audio_transcription: { model: "whisper-1" },
                  },
                }),
              );
            } catch (err) {
              console.error("Failed to configure session:", err);
            }
          });
        }
      } catch (err) {
        console.error("Failed to configure session:", err);
      }

      console.log("âœ… Voice-to-text recording started");
    } catch (error) {
      console.error("âŒ Failed to start voice-to-text:", error);
      setIsRecording(false);
      setIsListening(false);
      onError?.("ìŒì„± ì¸ì‹ ì‹œìž‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: " + (error as Error).message);
    }
  }, [isRecording, onTranscript, onError]);

  const stopRecording = useCallback(() => {
    console.log("ðŸ›‘ Stopping voice-to-text recording...");

    try {
      voiceConnRef.current?.stop();
    } catch (err) {
      console.error("Error stopping voice connection:", err);
    }

    voiceConnRef.current = null;
    setIsRecording(false);
    setIsListening(false);

    console.log("âœ… Voice-to-text recording stopped");
  }, []);

  const clearTranscript = useCallback(() => {
    setTranscriptText("");
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (voiceConnRef.current) {
        stopRecording();
      }
    };
  }, [stopRecording]);

  return {
    isRecording,
    isListening,
    transcriptText,
    startRecording,
    stopRecording,
    clearTranscript,
  };
};
