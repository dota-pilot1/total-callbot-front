import { useState, useRef, useEffect } from "react";
import { voiceApi } from "../api/voice";
import { connectRealtimeVoice, type VoiceConnection } from "../lib/realtime";
import type { CHARACTER_LIST } from "../../character/characters";

export interface UseVoiceConnectionOptions {
  speechLang: "ko" | "en";
  echoCancellation: boolean;
  noiseSuppression: boolean;
  autoGainControl: boolean;
  selectedVoice: string;
  personaCharacter: (typeof CHARACTER_LIST)[number];
  personaGender: "male" | "female";
  onUserMessage?: (text: string) => void;
  onAssistantMessage?: (text: string) => void;
}

export interface UseVoiceConnectionReturn {
  // ìƒíƒœë“¤
  voiceEnabled: boolean;
  isRecording: boolean;
  isListening: boolean;
  isResponding: boolean;
  isPaused: boolean;
  voiceConn: VoiceConnection | null;
  audioRef: React.RefObject<HTMLAudioElement | null>;

  // ì•¡ì…˜ë“¤
  startVoice: () => Promise<void>;
  stopVoice: () => void;
  pauseVoiceInput: () => void;
  resumeVoiceInput: () => void;
  setVoiceEnabled: (enabled: boolean) => void;
  sendVoiceMessage: (message: string) => void;
}

/**
 * OpenAI Realtime API ê¸°ë°˜ ì‹¤ì‹œê°„ AI ìŒì„± ëŒ€í™” í›…
 *
 * ì‚¬ìš©ì ìŒì„± â†’ ìŒì„±ì¸ì‹ â†’ AI ì‘ë‹µ ìƒì„± â†’ ìŒì„±í•©ì„± â†’ ì¬ìƒ
 * ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í›…
 */
export const useVoiceConnection = (
  options: UseVoiceConnectionOptions,
): UseVoiceConnectionReturn => {
  const {
    speechLang,
    echoCancellation,
    noiseSuppression,
    autoGainControl,
    selectedVoice,
    personaCharacter,
    personaGender,
    onUserMessage,
    onAssistantMessage,
  } = options;

  // ìŒì„± ì—°ê²° ìƒíƒœë“¤
  const [voiceEnabled, setVoiceEnabled] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [voiceConn, setVoiceConn] = useState<VoiceConnection | null>(null);
  const [isListening, setIsListening] = useState(false);
  const [isResponding, setIsResponding] = useState(false);
  const [isPaused, setIsPaused] = useState(false);

  // Refs
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const isRespondingRef = useRef(false);
  const assistantPartialRef = useRef<string>("");
  const lastUserFinalRef = useRef<string>("");
  const lastAssistantFinalRef = useRef<string>("");

  // í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜
  const normalizeText = (s: string) => {
    try {
      if (!s || typeof s !== "string") return "";
      let normalized = s.normalize("NFC");
      normalized = normalized.replace(/[\uFFFD\u0000-\u001F]/g, "");
      normalized = normalized.replace(/\s+/g, " ").trim();
      return normalized;
    } catch (error) {
      console.warn("í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤íŒ¨:", error);
      return (s || "").trim();
    }
  };

  // AI í˜ë¥´ì†Œë‚˜ ì§€ì‹œì‚¬í•­ ìƒì„±
  const buildPersonaInstructions = () => {
    const genderNote =
      personaGender === "male"
        ? "Use a subtly masculine persona. "
        : "Use a subtly feminine persona. ";
    const voiceNote = selectedVoice ? `Voice: ${selectedVoice}. ` : "";
    const persona = personaCharacter
      ? `${personaCharacter.personality}\n${personaCharacter.background}`
      : "";

    // ê¸°ë³¸ì ìœ¼ë¡œ ì˜ì–´ë¡œ ë‹µë³€, í•œêµ­ì–´ ìš”ì²­ ì‹œì—ë§Œ í•œêµ­ì–´ ì‚¬ìš©
    const languageNote =
      "Be direct and straightforward. Keep replies to 1-2 sentences maximum. No unnecessary explanations or elaboration. Start with a brief self-introduction when first greeting. Respond primarily in English. If the user specifically requests Korean (í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜, í•œê¸€ë¡œ ë§í•´ì¤˜, etc.), then respond in Korean using formal, respectful language. ";

    return (
      `I am ${personaCharacter.name} (${personaCharacter.emoji}). ${genderNote}${voiceNote}` +
      languageNote +
      `I stay in character at all times. I avoid meta talk.\n\nMy persona notes:\n${persona}`
    );
  };

  // ìŒì„± ì—°ê²° ì‹œì‘
  const startVoice = async () => {
    if (voiceConn) return;

    try {
      const session = await voiceApi.createSession({
        lang: speechLang,
        voice: selectedVoice,
      });

      let micStream: MediaStream | null = null;

      const conn = await connectRealtimeVoice({
        token: session.token,
        model: session.model,
        audioElement: audioRef.current,
        voice: selectedVoice,
        audioConstraints: {
          echoCancellation,
          noiseSuppression,
          autoGainControl,
          channelCount: 1,
        },
        onEvent: (evt) => {
          const e: any = evt as any;
          const t = e?.type as string | undefined;
          if (t === "input_audio_buffer.speech_started") setIsListening(true);
          if (t === "input_audio_buffer.speech_stopped") setIsListening(false);
          if (t === "output_audio_buffer.started") {
            setIsResponding(true);
            isRespondingRef.current = true;
            try {
              micStream
                ?.getAudioTracks()
                ?.forEach((tr) => (tr.enabled = false));
            } catch {}
          }
          if (t === "response.done" || t === "output_audio_buffer.stopped") {
            setIsResponding(false);
            isRespondingRef.current = false;
            try {
              micStream?.getAudioTracks()?.forEach((tr) => (tr.enabled = true));
            } catch {}
          }
        },
        onUserTranscript: (text, isFinal) => {
          // ğŸš« ì¼ì‹œì •ì§€ ìƒíƒœì—ì„œëŠ” ì‚¬ìš©ì ìŒì„± ì²˜ë¦¬ ì™„ì „ ì°¨ë‹¨
          if (isPaused) {
            console.log(
              "ğŸ”‡ [BLOCKED] ì¼ì‹œì •ì§€ ìƒíƒœë¡œ ì¸í•´ ì‚¬ìš©ì ìŒì„± ë¬´ì‹œ:",
              text,
            );
            return;
          }
          if (isRespondingRef.current) return; // ì–´ì‹œìŠ¤í„´íŠ¸ ë°œí™” ì¤‘ ì „ì‚¬ ë¬´ì‹œ
          if (isFinal) {
            const finalText = normalizeText(text.trim());
            console.log("[ìŒì„± ë””ë²„ê·¸] ìµœì¢… í…ìŠ¤íŠ¸:", finalText);
            console.log("[ìŒì„± ë””ë²„ê·¸] ì´ì „ í…ìŠ¤íŠ¸:", lastUserFinalRef.current);

            // í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¶”ê°€ (ì¤‘ë³µ ì²´í¬ ì™„í™”)
            if (finalText && finalText.length > 0) {
              console.log("[ìŒì„± ë””ë²„ê·¸] ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ì¤‘:", finalText);
              // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì™¸ë¶€ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
              onUserMessage?.(finalText);
              lastUserFinalRef.current = finalText;
            } else {
              console.log("[ìŒì„± ë””ë²„ê·¸] í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ì„œ ìŠ¤í‚µ");
            }
          }
        },
        onAssistantText: (text, isFinal) => {
          // ğŸš« ì¼ì‹œì •ì§€ ìƒíƒœì—ì„œëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ìŒì„± ì²˜ë¦¬ë„ ì™„ì „ ì°¨ë‹¨
          if (isPaused) {
            console.log(
              "ğŸ”‡ [BLOCKED] ì¼ì‹œì •ì§€ ìƒíƒœë¡œ ì¸í•´ ì–´ì‹œìŠ¤í„´íŠ¸ ìŒì„± ë¬´ì‹œ:",
              text,
            );
            return;
          }
          if (isFinal) {
            const finalText = normalizeText(
              assistantPartialRef.current || text,
            );
            const normalizedFinal = normalizeText(finalText);
            const normalizedLast = normalizeText(lastAssistantFinalRef.current);
            if (normalizedFinal && normalizedFinal !== normalizedLast) {
              // AI ë©”ì‹œì§€ë¥¼ ì™¸ë¶€ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
              onAssistantMessage?.(finalText);
              lastAssistantFinalRef.current = finalText.trim();
            }
            assistantPartialRef.current = "";
          } else {
            assistantPartialRef.current += text;
          }
        },
      });

      setVoiceConn(conn);
      try {
        micStream = conn.localStream;
      } catch {}
      setIsRecording(true);

      // ì„¸ì…˜ ê¸°ë³¸ í¼ì†Œë‚˜ ì—…ë°ì´íŠ¸
      try {
        if (conn.dc && conn.dc.readyState === "open") {
          conn.dc.send(
            JSON.stringify({
              type: "session.update",
              session: {
                instructions: buildPersonaInstructions(),
              },
            }),
          );
        } else {
          conn.dc?.addEventListener("open", () => {
            try {
              conn.dc?.send(
                JSON.stringify({
                  type: "session.update",
                  session: { instructions: buildPersonaInstructions() },
                }),
              );
            } catch {}
          });
        }
      } catch {}
    } catch (e) {
      console.error("ìŒì„± ì—°ê²° ì‹¤íŒ¨:", e);
    }
  };

  // ìŒì„± ì—°ê²° ì¢…ë£Œ
  const stopVoice = () => {
    try {
      voiceConn?.stop();
    } catch {}
    setVoiceConn(null);
    setIsRecording(false);
    setIsListening(false);
    setIsResponding(false);
    setIsPaused(false);
  };

  // ìŒì„± ì…ë ¥ ì¼ì‹œì •ì§€ (OpenAI Realtime API ê³µì‹ ì´ë²¤íŠ¸ ì‚¬ìš©)
  const pauseVoiceInput = () => {
    try {
      if (voiceConn?.dc && voiceConn.dc.readyState === "open" && !isPaused) {
        // ì§„í–‰ ì¤‘ì¸ ì‘ë‹µ ì·¨ì†Œ
        voiceConn.dc.send(
          JSON.stringify({
            type: "response.cancel",
          }),
        );

        // ì…ë ¥ ì˜¤ë””ì˜¤ ë²„í¼ í´ë¦¬ì–´
        voiceConn.dc.send(
          JSON.stringify({
            type: "input_audio_buffer.clear",
          }),
        );

        // ì¶œë ¥ ì˜¤ë””ì˜¤ ë²„í¼ í´ë¦¬ì–´
        voiceConn.dc.send(
          JSON.stringify({
            type: "output_audio_buffer.clear",
          }),
        );

        setIsPaused(true);
        setIsListening(false);
        setIsResponding(false);
        console.log("ìŒì„± ì…ë ¥ì´ ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤");
      }
    } catch (error) {
      console.error("ìŒì„± ì…ë ¥ ì¼ì‹œì •ì§€ ì‹¤íŒ¨:", error);
    }
  };

  // ìŒì„± ì…ë ¥ ì¬ê°œ
  const resumeVoiceInput = () => {
    try {
      if (voiceConn?.dc && voiceConn.dc.readyState === "open" && isPaused) {
        setIsPaused(false);
        console.log("ìŒì„± ì…ë ¥ì´ ì¬ê°œë˜ì—ˆìŠµë‹ˆë‹¤");
      }
    } catch (error) {
      console.error("ìŒì„± ì…ë ¥ ì¬ê°œ ì‹¤íŒ¨:", error);
    }
  };

  // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ìŒì„± ì—°ê²°ë¡œ ì „ì†¡
  const sendVoiceMessage = (message: string) => {
    try {
      if (voiceConn?.dc && voiceConn.dc.readyState === "open") {
        voiceConn.dc.send(
          JSON.stringify({
            type: "conversation.item.create",
            item: {
              type: "message",
              role: "user",
              content: [{ type: "input_text", text: message }],
            },
          }),
        );
        voiceConn.dc.send(
          JSON.stringify({
            type: "response.create",
            response: {
              modalities: ["audio", "text"],
              conversation: "auto",
              voice: selectedVoice,
            },
          }),
        );
      }
    } catch (e) {
      console.error("ìŒì„± ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨:", e);
    }
  };

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopVoice();
    };
  }, []);

  return {
    // ìƒíƒœë“¤
    voiceEnabled,
    isRecording,
    isListening,
    isResponding,
    isPaused,
    voiceConn,
    audioRef,

    // ì•¡ì…˜ë“¤
    startVoice,
    stopVoice,
    pauseVoiceInput,
    resumeVoiceInput,
    setVoiceEnabled,
    sendVoiceMessage,
  };
};
