import { useState, useRef, useEffect } from "react";
import { voiceApi } from "../../../shared/chatbot-utils/voice/api/voice";
import {
  connectRealtimeVoice,
  type VoiceConnection,
} from "../../../shared/chatbot-utils/voice/lib/realtime";
import type { CharacterSettings } from "./useCharacterState";

export interface UseCharacterVoiceOptions {
  settings: CharacterSettings;
  onUserMessage?: (text: string) => void;
  onAssistantMessage?: (text: string) => void;
  onUserSpeechStart?: () => void;
  onUserTranscriptUpdate?: (text: string, isFinal: boolean) => void;
}

export interface UseCharacterVoiceReturn {
  // ìƒíƒœë“¤
  isConnected: boolean;
  isListening: boolean;
  isResponding: boolean;

  // ì—°ê²° ê´€ë¦¬
  startConnection: () => Promise<void>;
  stopConnection: () => void;

  // ë©”ì‹œì§€ ì „ì†¡
  sendTextMessage: (message: string) => void;

  // ì˜¤ë””ì˜¤ ì°¸ì¡°
  audioRef: React.RefObject<HTMLAudioElement>;
}

/**
 * ìºë¦­í„° ì±—ë´‡ ì „ìš© ìŒì„± ì—°ê²° í›…
 * ê¸°ì¡´ useVoiceConnectionê³¼ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„
 */
export const useCharacterVoice = (
  options: UseCharacterVoiceOptions,
): UseCharacterVoiceReturn => {
  const {
    settings,
    onUserMessage,
    onAssistantMessage,
    onUserSpeechStart,
    onUserTranscriptUpdate,
  } = options;

  // ìƒíƒœë“¤
  const [isConnected, setIsConnected] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isResponding, setIsResponding] = useState(false);
  const [voiceConn, setVoiceConn] = useState<VoiceConnection | null>(null);

  // Refs
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const lastUserFinalRef = useRef<string>("");
  const lastAssistantFinalRef = useRef<string>("");
  const assistantPartialRef = useRef<string>("");

  /**
   * ìºë¦­í„° í˜ë¥´ì†Œë‚˜ ì§€ì‹œì‚¬í•­ ìƒì„±
   * í˜„ì¬ ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ OpenAI ì§€ì‹œì‚¬í•­ ìƒì„±
   */
  const buildCharacterInstructions = (): string => {
    const { character, gender, voice } = settings;

    console.log("ğŸ­ [buildCharacterInstructions] ì§€ì‹œì‚¬í•­ ìƒì„±:");
    console.log("  - character:", character);
    console.log("  - gender:", gender);
    console.log("  - voice:", voice);

    const genderNote =
      gender === "male"
        ? "Use a masculine persona. "
        : "Use a feminine persona. ";

    const voiceNote = `Voice: ${voice}. `;

    const characterInfo = `${character.personality}\n\n${character.background}`;

    const languageNote =
      "Stay in character and respond naturally in English. " +
      "If the user speaks in another language, understand their intent but respond as your character would, in English. " +
      "Keep responses conversational and direct. Maximum 1-2 sentences. ";

    const firstMessageNote = character.firstMessage
      ? `Your first message must be: "${character.firstMessage}". `
      : "";

    const instructions =
      `You are ${character.name} (${character.emoji}). ${genderNote}${voiceNote}` +
      firstMessageNote +
      `Character details: ${characterInfo}. ` +
      languageNote +
      `Always stay in character as ${character.name}.`;

    console.log("ğŸ­ [buildCharacterInstructions] ìƒì„±ëœ ì§€ì‹œì‚¬í•­:");
    console.log(instructions);

    return instructions;
  };

  /**
   * í…ìŠ¤íŠ¸ ì •ê·œí™”
   */
  const normalizeText = (text: string): string => {
    try {
      if (!text || typeof text !== "string") return "";
      let normalized = text.normalize("NFC");
      normalized = normalized.replace(/[\uFFFD\u0000-\u001F]/g, "");
      normalized = normalized.replace(/\s+/g, " ").trim();
      return normalized;
    } catch (error) {
      console.warn("í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹¤íŒ¨:", error);
      return (text || "").trim();
    }
  };

  /**
   * ìºë¦­í„°ì˜ ì²« ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
   */
  const sendFirstMessage = (conn: VoiceConnection) => {
    if (!conn.dc || conn.dc.readyState !== "open") {
      console.warn("ğŸ­ [sendFirstMessage] ë°ì´í„° ì±„ë„ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ");
      return;
    }

    try {
      const instructions = buildCharacterInstructions();

      // ì²« ë©”ì‹œì§€ ìƒì„± ìš”ì²­
      conn.dc.send(
        JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["text", "audio"],
            instructions:
              instructions +
              " Start the conversation with your first message as described in your character instructions. Introduce yourself naturally and greet the user warmly.",
          },
        }),
      );

      console.log("ğŸ­ [sendFirstMessage] ìºë¦­í„° ì²« ì¸ì‚¬ ë©”ì‹œì§€ ìš”ì²­ ì „ì†¡");
    } catch (error) {
      console.error("ğŸ­ [sendFirstMessage] ì²« ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨:", error);
    }
  };

  /**
   * ìŒì„± ì—°ê²° ì‹œì‘
   */
  const startConnection = async () => {
    if (voiceConn) {
      console.log("ğŸ­ [startConnection] ì´ë¯¸ ì—°ê²°ë˜ì–´ ìˆìŒ");
      return;
    }

    console.log("ğŸ­ [startConnection] ìŒì„± ì—°ê²° ì‹œì‘");

    try {
      // 1. ì„¸ì…˜ ìƒì„±
      const session = await voiceApi.createSession({
        lang: "en", // ì˜ì–´ ê³ ì •
        voice: settings.voice,
      });

      console.log("ğŸ­ [startConnection] ì„¸ì…˜ ìƒì„± ì™„ë£Œ:", session);

      // 2. Realtime ì—°ê²°
      const conn = await connectRealtimeVoice({
        token: session.token,
        model: session.model,
        audioElement: audioRef.current,
        voice: settings.voice,
        audioConstraints: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          channelCount: 1,
        },
        onEvent: (evt) => {
          const e: any = evt as any;
          const eventType = e?.type as string | undefined;

          if (eventType === "input_audio_buffer.speech_started") {
            setIsListening(true);
            onUserSpeechStart?.();
          }
          if (eventType === "input_audio_buffer.speech_stopped") {
            setIsListening(false);
          }
          if (
            eventType === "response.audio_transcript.started" ||
            eventType === "response.started"
          ) {
            setIsResponding(true);
          }
          if (
            eventType === "response.done" ||
            eventType === "response.audio_transcript.done"
          ) {
            setIsResponding(false);
          }
        },
        onUserTranscript: (text, isFinal) => {
          onUserTranscriptUpdate?.(text, isFinal);

          if (isFinal) {
            const finalText = normalizeText(text.trim());
            console.log("[ìŒì„± ë””ë²„ê·¸] ìµœì¢… í…ìŠ¤íŠ¸:", finalText);

            if (
              finalText &&
              finalText.length > 0 &&
              finalText !== lastUserFinalRef.current
            ) {
              console.log("[ìŒì„± ë””ë²„ê·¸] ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:", finalText);
              onUserMessage?.(finalText);
              lastUserFinalRef.current = finalText;

              // ì‘ë‹µ ìƒì„± ìš”ì²­
              setTimeout(() => {
                try {
                  conn?.dc?.send(
                    JSON.stringify({
                      type: "response.create",
                      response: {
                        modalities: ["text", "audio"],
                        instructions: buildCharacterInstructions(),
                      },
                    }),
                  );
                  console.log("[ìŒì„± ë””ë²„ê·¸] ì‘ë‹µ ìƒì„± ìš”ì²­ ì „ì†¡");
                } catch (e) {
                  console.error("[ìŒì„± ë””ë²„ê·¸] ì‘ë‹µ ìƒì„± ìš”ì²­ ì‹¤íŒ¨:", e);
                }
              }, 1000);
            }
          }
        },
        onAssistantText: (text, isFinal) => {
          if (isFinal) {
            const finalText = normalizeText(
              assistantPartialRef.current || text,
            );
            const normalizedFinal = normalizeText(finalText);
            const normalizedLast = normalizeText(lastAssistantFinalRef.current);

            if (normalizedFinal && normalizedFinal !== normalizedLast) {
              onAssistantMessage?.(finalText);
              lastAssistantFinalRef.current = finalText.trim();
            }
            assistantPartialRef.current = "";
          } else {
            assistantPartialRef.current += text;
          }
        },
      });

      setVoiceConn(conn);
      setIsConnected(true);

      console.log("ğŸ­ [startConnection] Realtime ì—°ê²° ì™„ë£Œ");

      // 3. ìºë¦­í„° ì§€ì‹œì‚¬í•­ ì„¤ì •
      const setupCharacter = () => {
        if (conn.dc && conn.dc.readyState === "open") {
          const instructions = buildCharacterInstructions();
          const sessionConfig = {
            type: "session.update",
            session: {
              instructions: instructions,
              turn_detection: {
                type: "server_vad",
                threshold: 0.6,
                prefix_padding_ms: 300,
                silence_duration_ms: 800,
                create_response: false,
              },
            },
          };

          console.log("ğŸ­ [startConnection] ìºë¦­í„° ì„¤ì • ì „ì†¡:");
          console.log(JSON.stringify(sessionConfig, null, 2));

          conn.dc.send(JSON.stringify(sessionConfig));
          console.log("ğŸ­ [startConnection] ìºë¦­í„° ì„¤ì • ì „ì†¡ ì™„ë£Œ");

          // ìºë¦­í„° ì„¤ì • í›„ ì²« ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡ (ì•½ê°„ì˜ ì§€ì—°)
          setTimeout(() => {
            sendFirstMessage(conn);
          }, 1000);
        }
      };

      // ë°ì´í„° ì±„ë„ì´ ì—´ë¦¬ë©´ ìºë¦­í„° ì„¤ì •
      if (conn.dc && conn.dc.readyState === "open") {
        setupCharacter();
      } else {
        conn.dc?.addEventListener("open", setupCharacter);
      }
    } catch (error) {
      console.error("ğŸ­ [startConnection] ì—°ê²° ì‹¤íŒ¨:", error);
      setIsConnected(false);
    }
  };

  /**
   * ìŒì„± ì—°ê²° ì¢…ë£Œ
   */
  const stopConnection = () => {
    console.log("ğŸ­ [stopConnection] ì—°ê²° ì¢…ë£Œ");

    try {
      voiceConn?.stop();
    } catch (error) {
      console.error("ì—°ê²° ì¢…ë£Œ ì‹¤íŒ¨:", error);
    }

    setVoiceConn(null);
    setIsConnected(false);
    setIsListening(false);
    setIsResponding(false);
  };

  /**
   * í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
   */
  const sendTextMessage = (message: string) => {
    if (!voiceConn?.dc || voiceConn.dc.readyState !== "open") {
      console.error("ğŸ­ [sendTextMessage] ì—°ê²°ì´ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ");
      return;
    }

    try {
      voiceConn.dc.send(
        JSON.stringify({
          type: "conversation.item.create",
          item: {
            type: "message",
            role: "user",
            content: [{ type: "input_text", text: message }],
          },
        }),
      );

      voiceConn.dc.send(
        JSON.stringify({
          type: "response.create",
          response: {
            modalities: ["audio", "text"],
            instructions: buildCharacterInstructions(),
          },
        }),
      );

      console.log("ğŸ­ [sendTextMessage] í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡:", message);
    } catch (error) {
      console.error("ğŸ­ [sendTextMessage] ì „ì†¡ ì‹¤íŒ¨:", error);
    }
  };

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì—°ê²° í•´ì œ
  useEffect(() => {
    return () => {
      stopConnection();
    };
  }, []);

  return {
    // ìƒíƒœë“¤
    isConnected,
    isListening,
    isResponding,

    // ì—°ê²° ê´€ë¦¬
    startConnection,
    stopConnection,

    // ë©”ì‹œì§€ ì „ì†¡
    sendTextMessage,

    // ì˜¤ë””ì˜¤ ì°¸ì¡°
    audioRef,
  };
};
